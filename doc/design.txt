Byte code enhancement - activate
================================
Activation should be called only once at the beginning of every public method, regardless
of whether it uses persistent fields or not. They might be accessed by private/protected methods.
Otherwise it would be hard, also because persistent fields might be access via local variables 
(int v = _v).

Exceptions
==========
JDOUserException and similar should always be thrown via DBLogger.newUser(). This will allow future
separation of JDO and ZooDB.
The schema API should not throw and JDOxxx exceptions at all because it is not part of the JDO spec.

Queries
=======
For String-queries, the policy in ZooDB is that for ordering, 'null' is smaller than any other 
String. See query result processor. 

Schema Generic Objects
======================
See issue #27. Does it make sense to make existence of GO and PC mutually exclusive? Whenever one is
requested, the other would be destroyed (GO) or invalidated (PC). 

Schema Evolution
================
Maybe we need to create an OID->Schema-OID index?
Due to schema evolution, the Schema-OID in serialized references may be out-dated with respect to
the referenced object. Generally, it may be impossible to create a hollow object from the OID.
Alternative: look up the schema and create a hollow of the latest version?!?!?


Schema Evolution Requirements (2013-01)
=======================================
- Allow full schema evolution with out Java classes (construct classes and instantiate objects)
- Constant initializers for new fields
- Allow reading/writing of SCOs as byte[] (and possibly other attributes)
- Deferred evolution. Do not evolve all object immediately, but allow deferring evolution to 
  loading of object.
--> Do not require evolution for simple renaming. Renaming should not trigger rewriting of objects!

Results: 
- When schema evolution occurs, databases needs to store:
    - initialization values
    - Possibly evolution steps (could be reconstructed, but why bother?) 


COW / Failure tolerance
=======================
What happens if commit fails between data commit and root-page-commit? Or during data commit?
Any newly allocated pages would be lost, unless the rootpage contains would contain the right
file-size. A subsequent write attempt would then allocate new pages not at the end of the file, 
but at the position stored in the rootpage. -> IMPLEMENTED.


COW / Large DB / SSD
====================
COW storage has the advantage that data is written only once. Unfortunately, this can be a 
disadvantage in expensive systems which outsource the log files to high performance drives, which
allows the ODBMS to return from commit after the log files are flushed, real database updates
can be delayed. The COW approach means that we always have to wait for the primary disk to flush.

Luckily this problem does not exist on small devices which do not have separate high-speed disk,
furthermore. It also does not exist where SSD are used for the whole database.


Paging
======

Paging is mainly exploited for indices.

Deserialize whole page???
Earlier, paging was also used to compress object, in particular, _usedObjects were re-used for
all object written in one stream. However, this disallowed random access as it happens during 
queries.
Especially with using multiple consequtive pages even for small objects (as we do currently),
this would require reading all previous object to ensure completeness of _usedObjects.


Indices
-------


Objects
-------
Multi-page objects. 
Separated by class.
All objects from a class in a continuous stream of pages. 



Transactional consistency / COW vs general(trans-trans-actional?) consistency
=============================================================================
There are several types of consistency: 
A) Internal to a transaction
B) Across transaction boundaries within a session
C) Parallel sessions/transactions.

First C) can be solved with locking or with COW (for example to create a consistent backup on a
running database).

B): The main example here are query-results/extents that should be valid across transactions.
    For example getting the class extent, iterating over all objects, getting a value from them 
    (just to have a reason not to use deletePersistentAll(), delete them one by one and
    commit() every few objects.
    Here the iterator should have following properties:
    - be valid across commit() (rollback???)
    - do not update (do not include objects that were not present when the extent was created).
      Why not?
    - Reset???
    - Invalidate? Smart invalidate? -> Invalidate only if iterator is affected...?????
    
    
A) Within a transaction, all field-indices should be aware of local non-committed changes (unless
   we use ignoreCache). To do this we need to keep local indices that override the database index
   (I think at the moment we just parse the whole cache??).
   Rule: All query-results/extents / indices should be consistent at all times. Critical conditions:
   - An object is modified (local index update?, update existing result-sets?????? .. probably no)
     Index update: Yes. Results should not need to be updated after they are created... ???
     Results: Filter out objects that changed later on, but do not include new objects.
   - An object is deleted: Index update, results should filter this out.
   - An objects is created: Index updates, results should not show it (guarantee to not show it?)
   Rules:
   - Results are snap-shots that are allowed to get out of sync.
   - Indices should be updated to allow any new queries/extents to be correct
   - Index-iterators are like results (Extent is an index iterator) -> Filter out non-matches and
     deleted objects. This can not be guaranteed. The object may change/be deleted while being
     returned by next(). 
     Do not guarantee to exclude new (newly matching) objects.
   -> Behaviour undefined?
   -> Or invalidate all results? -> Smart invalidation, invalidate only if result is affected.(!!!!) 
     
   -> For now we refresh() the pos-index upon commit(). This should always be sufficient,
      because one would expect that a loop over an extent only modifies/deletes objects
      retrieved via the iterator. Because of that, the iterator can not miss out on objects.
      However, a modified objects may be returned again, because it may have been copied to 
      a higher page in the DB.


Usage of indices
================

Main indices
------------
There is one main index that maps from OID to page/offset. This is important to look up references,
which are implemented through OIDs.

There is a reverse indexing mechanism: For each Class there is an index from page/offset to OID.
This is important for class based operations: Queries and schema evolution.
For queries it is only important, if no attribute in the query is indexed. It allows finding all
instances of a particular class, without having to look up all objects. Discussion: It would also
be possible (but less useful) to have an index from OID to Schema-ID. 
On rotational drives (e.g. no SSD), this index also allows pure sequential read. 

The latter indexing can also be used as (or to support an) free-space manager.
 
QUESTIONS
A problem that 
remains to be solved here is concerning objects that span multiple pages. How can the secondary
object pages be recognized? Store them as well in the Schema-index?

NON-UNIQUE ordering
If we decide to store all pages that belong to an OID in the index, the index contains many 
OID-page pairs. But which is the starting page? Simply the first one. Even with using a freespace
manager, the pages allocated during the write process for a single object should always be
ascending. That is, because the freespace manager should never be updated during the writing of
(a single) object.



Query-indices:
--------------


COW-indices
-----------
COW indices allow creating an iterator that represents a consistent snapshot of an index. This is
important for the (optional!) JDO feature, that a query result should not change, even if 
matching object are added or removed.
It may also be useful for concurrency, e.g. managing multiple parallel transactions.


Discussion of BitMap indices
===============================
A binary map does not store keys, but only if a certain key is used or not.
By definition, a BitMap is a unique index.
Storing the keys is quite efficient, for example 64 keys in one LONG. The values are stored in a 
separate array. the number of values can be stored separately or derived from the number of bits in 
the batch.
The batches are organized in a hierarchy. 


Schema
======
We assume that most schemas are used in a particular session. Therefore they are all loaded at
database connection time.

Lessons learned
===============
- String.getBytes() and 'new String(byte[])' are extremely slow, they even include synchronization.
  Use 'new String(char[])' instead.
  
- ArrayList is faster than BucketList! Even though ArrayList doesn't scale because of array copying,
  it is still much faster than BucketList for 1.000.000 entries.
  
- Boolean is not a Number!!!
